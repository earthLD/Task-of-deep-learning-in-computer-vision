{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "from ops import *\n",
    "from utils import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "config =  tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "#tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to train GAN for generating faces and then we will make fun playing with it. Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the “adversarial”). One neural network, called the generator, generates new faces, while the other, the discriminator,  decides whether each instance of face it reviews belongs to the actual training dataset or not.\n",
    "\n",
    "Firstly download aligned faces of celebrities from here <a href=\"https://yadi.sk/d/xjuClJJH3MAVXh\">link</a> and extract them into folder near ipython notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant variables below depends on your dataset and choosing of architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_dim = 100\n",
    "image_size=64\n",
    "batch_size=64\n",
    "sample_size=64\n",
    "owres=8\n",
    "gf_dim=64\n",
    "df_dim=64\n",
    "gfc_dim=1024\n",
    "dfc_dim=1024\n",
    "c_dim=3\n",
    "lam=0.1\n",
    "\n",
    "image_shape = [image_size, image_size, c_dim]\n",
    "d_bns = [batch_norm(name='d_bn{}'.format(i,)) for i in range(4)]\n",
    "\n",
    "log_size = int(math.log(image_size) / math.log(2))\n",
    "g_bns = [ batch_norm(name='g_bn{}'.format(i,)) for i in range(log_size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define GAN. To do it, we need to define generator, discriminator and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tips on the architecture of the generator:\n",
    "1. The deeper is convolution, the less filters is using.\n",
    "2. Apply deconvolutions-relu layers to achieve input image shape.\n",
    "3. Use batch normalization before nonlinearity for speed and stability of learning.\n",
    "4. Use tanh activation at the end of network (in this case images should be scaled to [-1, 1])\n",
    "5. To force generator not to collapse and produce different outputs initialize bias with zero (see linear layer).\n",
    "\n",
    "Other useful tips: https://github.com/soumith/ganhacks. Example of architecture see below. You may also use defined layers from ops.py. <b> Please, use names for layers started with \"g\\_\" for generator and \"d_\" for discriminator.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/carpedm20/DCGAN-tensorflow/master/DCGAN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing generator function (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE  ) as scope:\n",
    "        z_, h0_w, h0_b = linear(z, gf_dim*8*4*4, 'g_h0_lin', with_w=True)\n",
    "\n",
    "        hs = [None]\n",
    "        hs[0] = tf.reshape(z_, [-1, 4, 4, gf_dim * 8])\n",
    "        hs[0] = tf.nn.relu(g_bns[0](hs[0], is_training))\n",
    "\n",
    "        i = 1 # Iteration number.\n",
    "        depth_mul = 8  # Depth decreases as spatial component increases.\n",
    "        size = 8  # Size increases as depth decreases.\n",
    "\n",
    "        while size < image_size:\n",
    "            hs.append(None)\n",
    "            name = 'g_h{}'.format(i)\n",
    "            hs[i], _, _ = deconv2d(hs[i-1],\n",
    "                [batch_size, size, size, gf_dim*depth_mul], name=name, with_w=True)\n",
    "            hs[i] = tf.nn.relu(g_bns[i](hs[i], is_training))\n",
    "\n",
    "            i += 1\n",
    "            depth_mul //= 2\n",
    "            size *= 2\n",
    "\n",
    "        hs.append(None)\n",
    "        name = 'g_h{}'.format(i)\n",
    "        hs[i], _, _ = deconv2d(hs[i - 1],\n",
    "            [batch_size, size, size, 3], name=name, with_w=True)\n",
    "\n",
    "        return tf.nn.tanh(hs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define discriminator. Discriminator takes 3d tensor as input and outputs one number - probability that this is an image.\n",
    "\n",
    "Some advice for discriminator's architecture:\n",
    "1. Use batch normalization between convolutions and nonlinearities.\n",
    "2. Use leaky relu with the leak about 0.2.\n",
    "3. The deeper the layer, the more filters you can use.\n",
    "\n",
    "If you use batch normalization, please define every layer in their own scope and pass is_training parameter there. Or you may use class of batch normalization from ops.py. Do not forget to fratten tensor after the convolution blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing discriminator function (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(image, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE  ) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        h0 = lrelu(conv2d(image, df_dim, name='d_h0_conv'))\n",
    "        h1 = lrelu(d_bns[0](conv2d(h0, df_dim*2, name='d_h1_conv'), is_training))\n",
    "        h2 = lrelu(d_bns[1](conv2d(h1, df_dim*4, name='d_h2_conv'), is_training))\n",
    "        h3 = lrelu(d_bns[2](conv2d(h2, df_dim*8, name='d_h3_conv'), is_training))\n",
    "        h4 = linear(tf.reshape(h3, [-1, 8192]), 1, 'd_h4_lin')\n",
    "\n",
    "        return tf.nn.sigmoid(h4), h4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "images = tf.placeholder(\n",
    "    tf.float32, [None] + image_shape, name='real_images')\n",
    "\n",
    "z = tf.placeholder(tf.float32, [None, z_dim], name='z')\n",
    "z_sum = tf.summary.histogram(\"z\", z)\n",
    "\n",
    "G = generator(z)\n",
    "\n",
    "D, D_logits = discriminator(images)\n",
    "\n",
    "D_, D_logits_ = discriminator(G, reuse=True)\n",
    "\n",
    "d_sum = tf.summary.histogram(\"d\", D)\n",
    "d__sum = tf.summary.histogram(\"d_\", D_)\n",
    "G_sum = tf.summary.image(\"G\", G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write definition of loss funstions according to formulas:\n",
    "$$ D\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m}[\\log{D(x_i)} + \\log{(1 - D(G(z_i)))}]$$\n",
    "$$ G\\_loss = \\frac{1}{m} \\sum_{i=1}^{m} \\log{(1 - D(G(z_i)))}$$\n",
    "\n",
    "Or for better learning you may try other loss for generator:\n",
    "$$ G\\_loss = \\frac{-1}{m} \\sum_{i=1}^{m} \\log{(D(G(z_i)))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing loss functions (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        You code goes here. Define discriminator and generator losses\n",
    "\"\"\"\n",
    "d_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits,\n",
    "                                            labels=tf.ones_like(D)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_,\n",
    "                                            labels=tf.zeros_like(D_)))\n",
    "g_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logits_,\n",
    "                                            labels=tf.ones_like(D_)))\n",
    "\n",
    "d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss_sum = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "d_loss_sum = tf.summary.scalar(\"d_loss\", d_loss)\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizers. We use different optimizers for discriminator and generator, so we needed a separate prefix for the discriminator and generator variables (g_ for generator, d_ for disciminator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0002\n",
    "beta1=0.5\n",
    "\n",
    "d_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1) \\\n",
    "                  .minimize(d_loss, var_list=d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(learning_rate, beta1=beta1) \\\n",
    "                  .minimize(g_loss, var_list=g_vars)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def center_crop(x, crop_h, crop_w=None):\n",
    "    # The original images are 218*178. If I crop directly 64*64 at the center, I will probably get only\n",
    "    # a small part of a face, and the result won't be good. So I crop 128*128, and I resize at 64*64\n",
    "    if crop_w is None:\n",
    "        crop_w = crop_h\n",
    "    h, w = x.shape[:2]\n",
    "    #j = int(round((h - crop_h)/2.))\n",
    "    #i = int(round((w - crop_w)/2.))\n",
    "    j = h//2 - crop_h\n",
    "    i = w//2 - crop_w\n",
    "    result = cv2.resize(x[j:j+crop_h*2, i:i+crop_w*2], (crop_h, crop_w))\n",
    "    return result\n",
    "\n",
    "def transform(image, npx=64, is_crop=True):\n",
    "    # npx : # of pixels width/height of image\n",
    "    if is_crop:\n",
    "        cropped_image = center_crop(image, npx)\n",
    "    else:\n",
    "        cropped_image = image\n",
    "    return np.array(cropped_image)/127.5 - 1.\n",
    " \n",
    "def get_image(image_path, image_size, is_crop=True):\n",
    "    return transform(imread(image_path), image_size, is_crop)\n",
    "\n",
    "DATA_PATH = '/data/luodan_data/img_align_celeba/' # Path to the dataset with celebA faces\n",
    "data = glob(os.path.join(DATA_PATH, \"*.jpg\"))\n",
    "N = len(data)+1\n",
    "images = np.zeros((N, image_size, image_size, 3), dtype=np.float32)\n",
    "\n",
    "i = 0 \n",
    "for filepath in data:\n",
    "    filename = filepath.replace(DATA_PATH,'')\n",
    "    imgid = int(filename.replace('.jpg',''))\n",
    "    images[imgid] = get_image(filepath,image_size) \n",
    "np.save('images.npy',images)\n",
    "del(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = glob(os.path.join(DATA_PATH, \"*.png\"))\n",
    "#assert(len(data) > 0), \"Length of training data should be more than zero\"\n",
    "\n",
    "data = np.load('images.npy')\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202600, 64, 64, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for training and evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(checkpoint_dir, step):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    saver.save(sess,\n",
    "                    os.path.join(checkpoint_dir, 'FVK_DCGAN_TF'),\n",
    "                    global_step=step)\n",
    "\n",
    "def load(checkpoint_dir):\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN (1 point + 2 for good results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4ab63703d0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         print(\"NEW MODEL\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mbatch_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "epoch=2\n",
    "checkpoint_dir=\"checkpoint\"\n",
    "sample_size = 64\n",
    "sample_dir = 'samples'\n",
    "\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "\n",
    "    try:\n",
    "        tf.global_variables_initializer().run()\n",
    "    except:\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "    g_sum = tf.summary.merge(\n",
    "        [z_sum, d__sum, G_sum, d_loss_fake_sum, g_loss_sum])\n",
    "    d_sum = tf.summary.merge(\n",
    "        [z_sum, d_sum, d_loss_real_sum, d_loss_sum])\n",
    "    writer = tf.summary.FileWriter(\"./logs\", sess.graph)\n",
    "\n",
    "    sample_z = np.random.normal(loc=0.,scale=1., size=(sample_size , z_dim))    \n",
    "    sample_images = data[0:sample_size]\n",
    "\n",
    "    counter = 1\n",
    "    start_time = time.time()\n",
    "\n",
    "#     if load(checkpoint_dir):\n",
    "#         print(\"OLD MODEL\")\n",
    "#     else:\n",
    "#         print(\"NEW MODEL\")\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "        length = data.shape[0]\n",
    "        batch_idxs = length // batch_size\n",
    "\n",
    "        for idx in range(batch_idxs):\n",
    "            \n",
    "            batch_images = data[idx*batch_size:(idx+1)*batch_size]\n",
    "            \n",
    "            batch_z = np.random.normal(loc=0., scale=1., size=[batch_size, z_dim]) \\\n",
    "                        .astype(np.float32)\n",
    "\n",
    "            # Update D network\n",
    "            _, summary_str = sess.run([d_optim, d_sum],\n",
    "                feed_dict={ images: batch_images, z: batch_z, is_training: True })\n",
    "            #writer.add_summary(summary_str, counter)\n",
    "\n",
    "            # Update G network\n",
    "            _, summary_str = sess.run([g_optim, g_sum],\n",
    "                feed_dict={ z: batch_z, is_training: True })\n",
    "            #writer.add_summary(summary_str, counter)\n",
    "\n",
    "            # Run g_optim twice to make sure that d_loss does not go to zero \n",
    "            _, summary_str = sess.run([g_optim, g_sum],\n",
    "                feed_dict={ z: batch_z, is_training: True })\n",
    "            #writer.add_summary(summary_str, counter)\n",
    "\n",
    "            errD_fake = d_loss_fake.eval({z: batch_z, is_training: False})\n",
    "            errD_real = d_loss_real.eval({images: batch_images, is_training: False})\n",
    "            errG = g_loss.eval({z: batch_z, is_training: False})\n",
    "\n",
    "            counter += 1\n",
    "            if np.mod(counter, 500) == 2:\n",
    "                print(\"Epoch: [{:2d}] [{:4d}/{:4d}] time: {:4.4f}, d_loss: {:.8f}, g_loss: {:.8f}\".format(\n",
    "                    epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG))\n",
    "                 # Update D network\n",
    "                summary_str = sess.run([d_sum],\n",
    "                    feed_dict={ images: batch_images, z: batch_z, is_training: True })\n",
    "                writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # Update G network\n",
    "                summary_str = sess.run([g_sum],\n",
    "                    feed_dict={ z: batch_z, is_training: True })\n",
    "                writer.add_summary(summary_str, counter)\n",
    "\n",
    "            if np.mod(counter, 5000) == 2:\n",
    "                save(checkpoint_dir, counter)\n",
    "\n",
    "#             if counter == 1  or np.mod(counter, 100) == 1:\n",
    "#                 samples, d_loss_s, g_loss_s = sess.run(\n",
    "#                     [G, d_loss, g_loss],\n",
    "#                     feed_dict={z: sample_z, images: sample_images, is_training: False}\n",
    "#                 )\n",
    "#                 save_images(samples, [8, 8],\n",
    "#                             './samples/train_{:02d}_{:04d}.png'.format(epoch, idx))\n",
    "#                 print(\"[Sample] d_loss: {:.8f}, g_loss: {:.8f}\".format(d_loss_s, g_loss_s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you generated something that looks like a face - it's cool! Add 2 points to your mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"512\" height=\"512\" autoplay>\n",
       "   <source src=\"fit.mp4\" type=\"video/mp4\">\n",
       "</video> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"512\" height=\"512\" autoplay>\n",
    "   <source src=\"fit.mp4\" type=\"video/mp4\">\n",
    "</video> \n",
    "<!-- EXECUTE THIS CELL TO WATCH THE VIDEO -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face interpolation (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpolate between faces: generate two vectors $z_1$ and $z_2$ and get a batch of vectors of the form $\\alpha\\cdot z_1 + (1- \\alpha)\\cdot  z_2, \\alpha \\in [0,1].$ Generate faces on them and look at results. The generator displays pictures in the range from -1 to 1, so use the inverse transform function from the file utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir=\"checkpoint\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    try:\n",
    "        tf.global_variables_initializer().run()\n",
    "    except:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "    load(checkpoint_dir)\n",
    "    sample_size = 64\n",
    "    sample_images = data[0:sample_size]\n",
    "    z1 = np.random.normal(loc=0.,scale=1., size=(sample_size , z_dim))\n",
    "    z2 = np.random.normal(loc=0.,scale=1., size=(sample_size , z_dim))\n",
    "    \n",
    "    for i,alpha in enumerate(np.linspace(0,1,30)):\n",
    "    \n",
    "        sample_z = alpha*z1 +(1-alpha)*z2\n",
    "\n",
    "        samples, d_loss_s, g_loss_s = sess.run(\n",
    "            [G, d_loss, g_loss],\n",
    "            feed_dict={z: sample_z, images: sample_images, is_training: False}\n",
    "        )\n",
    "        save_images(samples, [8, 8], './mixed%02d.png' % i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ffmpeg -framerate 1  -i mixed%02d.png -c:v libx264 -r 30 -pix_fmt yuv420p mixed.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"512\" height=\"512\" autoplay>\n",
       "   <source src=\"mixed.mp4\" type=\"video/mp4\">\n",
       "</video> \n",
       "<!-- EXECUTE THIS CELL TO WATCH THE VIDEO -->"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"512\" height=\"512\" autoplay>\n",
    "   <source src=\"mixed.mp4\" type=\"video/mp4\">\n",
    "</video> \n",
    "<!-- EXECUTE THIS CELL TO WATCH THE VIDEO -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a smile (1 point + 1 point for good results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make face smiling. Find several vectors z, such that the generator generates smiling faces and not. Five vectors in every group should be enough (but the more, the better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate \"smile vector\" as mean of vectors z with generated smile on it minus mean of vectors z with generated not smile on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the result of applying the smile vector: compare the results of generation before and after the addition of the smile vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    try:\n",
    "        tf.global_variables_initializer().run()\n",
    "    except:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "    load(checkpoint_dir)\n",
    "    sample_size = 64\n",
    "    sample_images = data[1100:sample_size]\n",
    "    #sample_z = np.random.uniform(-1,1, size=(sample_size, z_dim))\n",
    "    sample_z = np.random.normal(loc=0, scale=1.0, size=(sample_size, z_dim))\n",
    "\n",
    "    samples, d_loss_s, g_loss_s = sess.run(\n",
    "        [G, d_loss, g_loss],\n",
    "        feed_dict={z: sample_z, images: sample_images, is_training: False}\n",
    "    )\n",
    "    save_images(samples, [4,16], 'people.png')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='people.png'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src='people.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick it manualy\n",
    "smile_id = [1,6,12,13, 3*16, 16*3+15]\n",
    "no_smile_id = [2,16+11,14,16,16+14]\n",
    "man_id = [3,3*16+4,3*16+6,3*16+13 , 3*16+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smiling woman\n",
    "wo_smile = np.mean(sample_z[np.array(smile_id)], axis=0)\n",
    "#not smiling woman\n",
    "wo_no_smile = np.mean(sample_z[np.array(no_smile_id)], axis=0)\n",
    "#not smiling man\n",
    "man_no_smile = np.mean(sample_z[np.array(man_id)], axis=0)\n",
    "#smiling man\n",
    "man_smile = wo_smile - wo_no_smile +  man_no_smile\n",
    "man_smile_wn = np.zeros((sample_size,z_dim))\n",
    "man_smile_wn[:] = man_smile\n",
    "man_smile_wn += np.random.uniform(-.01, 0.01, size=(sample_size,z_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    try:\n",
    "        tf.global_variables_initializer().run()\n",
    "    except:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "    load(checkpoint_dir)\n",
    "    sample_size = 64\n",
    "    sample_images = data[0:sample_size]\n",
    "\n",
    "    samples, d_loss_s, g_loss_s = sess.run(\n",
    "        [G, d_loss, g_loss],\n",
    "        feed_dict={z:man_smile_wn , images: sample_images, is_training: False}\n",
    "    )\n",
    "    save_images(samples, [4, 16], 'man_smile.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='man_smile.png'>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src='man_smile.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If faces looks really cool, add bonus 1 point to your score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
